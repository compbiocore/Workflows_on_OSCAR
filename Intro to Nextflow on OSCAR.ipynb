{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4601bc",
   "metadata": {},
   "source": [
    "<h1><center>Basic Bioinformatics Workflows on OSCAR with Nextflow</center></h1>\n",
    " <center>Center for Computation and Visualization</center>\n",
    " <center>Center for Computational Biology of Human Disease - Computational Biology Core</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2fc35",
   "metadata": {},
   "source": [
    "## What is Nextflow?  \n",
    "\n",
    "Nextflow is a workflow management tool that allow users to easily write data-intensive computational **pipelines**. These pipelines, or workflows as they are also called, have the following key features:\n",
    "\n",
    "- Sequential processing of files\n",
    "- Usually requires more than one tool\n",
    "- Multiple programming languages\n",
    "- Most times each sample is processed individually\n",
    "- Compute resource intensive\n",
    "  - Alignment could take 16 cpus, 60 Gb RAM, 4-24 hours, 30Gb of disk space per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6649f1",
   "metadata": {},
   "source": [
    "## Why do we care about these pipelines? \n",
    "\n",
    "### Reason 1: Reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06abc40",
   "metadata": {},
   "source": [
    "<img src=\"./img/reproduce.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb742d3",
   "metadata": {},
   "source": [
    "The journal Nature published a survey that found that more than 70% of researchers have tried and failed to reproduce another scientist's experiments. This trend is hugely problematic because we then can't trust the findings from many studies enough to use them to make data-driven decisions. In short, we need tools and standards that help address the reproducibility crisis in science! \n",
    "\n",
    "Pipelines created with Snakemake and Nextflow incorporate version-control and state-of-the-art software tools, known as containers, to manage all software dependencies and create stable environments that can be trusted to reproduce analyses reliably and accurately. \n",
    "\n",
    "***Reproducibility is important for producing trustworthy research***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bbec0",
   "metadata": {},
   "source": [
    "### Reason 2: Portability\n",
    "\n",
    "#### What if we need to perform analyses with more resources?\n",
    "\n",
    "This type of scenario would require us to move our analyses to a different environment, for example, our High Performance Computing (HPC) cluster environment, OSCAR. \n",
    "\n",
    "An important feature of the Nextflow workflow management tool is that it enables users to scale any pipeline written on a personal computer to then run on an HPC cluster such as OSCAR. So now we can run our pipelines using high performance resources without having to change workflow definitions or hard-code a pipeline to a specific setup. As a result, **the code stays constant** across varying infrastructures, thereby allowing portability, easy collaboration, and avoiding lock-in. In short, we can easily move our multi-step analyses (i.e., pipelines) to any place we need them!\n",
    "\n",
    "***However, the trick is that setting up Nextflow on OSCAR requires a bit of configuration to get it running and many users at Brown aren't always comfortable using the terminal, setting up software, and dealing with software dependencies. So we want to not only offer users this tool and its benefits but also make its setup and use on OSCAR a bit user-friendly (and a lot more documented!).***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d46b76",
   "metadata": {},
   "source": [
    "## First Let's See How All This Works\n",
    "\n",
    "### Our Starting Point\n",
    "\n",
    "Say that we have a basic RNA-Seq pipeline that we need to perform on OSCAR with the following set of actions: \n",
    "\n",
    "<img src=\"./img/workflow2.png\" width=\"700\"/>\n",
    "\n",
    "<h2><center>What do you do??</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbe0a7",
   "metadata": {},
   "source": [
    "## A Naive Approach\n",
    "\n",
    "One solution would be to write a bunch of shell scripts that use various software tools to process the data in the ways we need. \n",
    "\n",
    "For example, if we needed to index a genome, perform an alignment, and then do transcript assembly, we could just write a shell script for each step - so a total of 3 shell scripts in this case - where we have various inputs and outputs and call different tools for each step. In this case, at a minimum, the tools needed would be: \"bowtie2\", \"Tophat2\", and \"cufflinks.\" Here is a rough example of what these scripts might look like. \n",
    "\n",
    "**Script 1: Indexing**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J bowtie_index\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate some_environment_with_bowtie2 \n",
    "genome_dir=\"/gpfs/data/cbc/project_folder\" \n",
    "bowtie2-build \"${genome_dir}/reference_sequence.fasta\" index_name \n",
    "```\n",
    "\n",
    "**Script 2: Alignment** \n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH -n 32\n",
    "#SBATCH -J align_rna\n",
    "#SBATCH --mem=198GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate some_environment_with_Tophat\n",
    "for sample in `ls /gpfs/data/cbc/project_folder/*_{1,2}.fq`\n",
    "do\n",
    "    tophat2 --GTF /path/to/bed.gff_file genome.index $sample\n",
    "done \n",
    "```\n",
    "\n",
    "\n",
    "**Script 3: Assembly**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH -n 8\n",
    "#SBATCH -J assembly_rna\n",
    "#SBATCH --mem=16GB\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu\n",
    "\n",
    "source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda\n",
    "conda activate some_environment_with_cufflinks\n",
    "for sample in `ls /gpfs/data/cbc/project_folder/bam_files`\n",
    "do\n",
    "cufflinks --no-update-check -q -G $annotation_gff_file $sample\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5d964",
   "metadata": {},
   "source": [
    "## Problems with the Naive Approach \n",
    "\n",
    "Using multiple shell scripts to create a makeshift pipeline will work, but it is **inefficient**, can **get complicated fast**, and there are a few **challenges you have to manage**, such as: \n",
    "\n",
    "* Making sure you have the appropriate software and all dependencies for each step in the analysis - this can be a lot to stay on top of if you have a pipeline with a lot of steps! (imagine a 10 step process)\n",
    "* **Portability!** Building and running on different machines is much more work\n",
    "* Specifying where your output will go \n",
    "* Calling in the appropriate input (which is often the output from a previous step) \n",
    "* Handling where log files go \n",
    "* More labor intensive - we have to stay on top of jobs and monitor when each step finishes and then run next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb9e7b",
   "metadata": {},
   "source": [
    "## A Better Approach: Using Workflow Managers \n",
    "\n",
    "The solution for processing your data in a much more efficient manner that handles the aforementioned issues is workflow management tools, such as Nextflow. Let's now learn how to use Nextflow with OSCAR..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372c107",
   "metadata": {},
   "source": [
    "<img src=\"./img/nextflow.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872e2d",
   "metadata": {},
   "source": [
    "### The Setup\n",
    "\n",
    "So the somewhat tricky part is setting up and configuring Nextflow for use on OSCAR. There can be a few challenges, especially for community users more unfamiliar with the terminal and with software installation and dependencies. These challenges are: \n",
    "\n",
    "1. When running Nextflow, we often do NOT want to re-invent the wheel and so instead we usually pull and execute existing pipelines from either GitHub or from nf-core (a community effort to collect a curated set of pipelines built using Nextflow), which interfaces with GitHub; as a result, the user needs to allow Nextflow access to GitHub. \n",
    "\n",
    "2. The user needs to specify the computational resources they wish to allocate to different pipelines or even specific tasks within pipelines and how this will happen (i.e., what workload manager will they be using?). Moreover, there are several sources of configuration specifications that can be used with Nextflow and they can conflict, so how does one resolve and manage this and easily set up the configuration for use with OSCAR? For example, resource specifications can be given to Nextflow from the following configuration sources: \n",
    "\n",
    "    a\\. Parameters specified in sbatch script via `#SBATCH` command or within `nextflow run` command \n",
    "    \n",
    "    b\\. Parameters provided using the -params-file option (take a yaml) \n",
    "    \n",
    "    c\\. Config file specified using the -c my_config option\n",
    "    \n",
    "    d\\. The config file named nextflow.config in the current directory\n",
    "    \n",
    "    e\\. The config file named nextflow.config in the workflow project directory\n",
    "    \n",
    "    f\\. The config file $HOME/.nextflow/config\n",
    "    \n",
    "    g\\. Values defined within the pipeline script itself (e.g. main.nf)\n",
    "    \n",
    "  When setting up the configuration, Nextflow automatically looks in all these different places and attempts to merge the configuration information; if there is conflicting configuration information, then the configuration source with the higher ranking overwrites the same configuration information from the lower ranking source. \n",
    "\n",
    "3. How to handle singularity containers and their mounting needs to be addressed; also suingularity caching is important, as jobs need enough storage space to not only store containers and but also all the temporary files created when converting Singularity containers from Docker images. Any issue here can crash the pipeline and create problems. \n",
    "\n",
    "4. User needs to set things up so that clearly manage and keep track of both slurm job output and pipeline output. \n",
    "\n",
    "To address these challenges, we created a user-friendly tool (essentially a user-friendly CLI) to assist users in setting up and configuring Nextflow for OSCAR and that automatically handles many of these aforementioned elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc5e51",
   "metadata": {},
   "source": [
    "###  Demo and Walkthrough\n",
    "\n",
    "For this portion, we will walkthrough the installation and setup guide to configure Nextflow and also illustrate how to use it on OSCAR. \n",
    "\n",
    "If you wish, you can follow along by going to this repo: https://github.com/compbiocore/Workflows_on_OSCAR\n",
    "\n",
    "All you need to do is git clone the `nextflow_setup` folder into your `HOME` directory and then follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4c1e",
   "metadata": {},
   "source": [
    "### Using Nextflow with an nf-core pipeline\n",
    "\n",
    "nf-core has many analysis pipelines we can use and so we need to identify the specific pipeline that is appropriate for our needs. For example, if we were analyzing RRBS data, we would need a pipeline that is appropriate to use for processing RRBS data. Heading over to https://nf-co.re/pipelines we can see that the **methylseq** pipeline will work for these types of data. We can view the details of this pipeline, such as all of its arguments and the steps it performs, by cliking on its link or visiting here: https://nf-co.re/methylseq \n",
    "\n",
    "**Note:** Visitng the pipeline's page and reviewing the pipeline's documentation is important because we need to know what arguments to use to make it run correctly. Once we've reviewed this and got an idea of how we need to run things, we can use the pipeline. \n",
    "\n",
    "We can simply create a bash script called **nextflow.sh** that leverages our Nextflow installation using something similar to this: \n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "###############################\n",
    "#                             #\n",
    "#   Nextflow with nf-core     #\n",
    "#                             #\n",
    "###############################\n",
    "\n",
    "##### 1.) Job Sumbission Options ######\n",
    "\n",
    "# Can change/add resources as needed \n",
    "\n",
    "# Logs\n",
    "#SBATCH --mail-user=jordan_lawson@brown.edu  \n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=%x-%j.out\n",
    "\n",
    "##### 2.) Run Nextflow #####\n",
    "\n",
    "# Activate Nextflow setup \n",
    "source $HOME/nextflow_env_username/bin/activate\n",
    "\n",
    "# Run nextflow \n",
    "nextflow run nf-core/methylseq -profile singularity --input \"/gpfs/data/ccvstaff/jlawson9/data_folder/*.fq.gz\" --single_end --genome Sscrofa10.2 -c $HOME/nextflow_setup/nextflow.config --outdir $HOME/scratch\n",
    "```\n",
    "\n",
    "Then we simply run everything on the cluster by typing ```sbatch nextflow.sh```\n",
    "\n",
    "Note in the above that we are running the nextflow program with the methylseq pipeline, as specified in the ```nf-core/methylseq``` argument that comes right after the ```nextflow run``` command. This tells nextflow to fetch and download the methylseq pipeline from nf-core. Some more important notes: \n",
    "\n",
    "- your log file for slurm will automtatically be placed in your HOME directory (unless otherwise specified). You can control where log files go using the ```-log``` flag followed by the path you wish to store the logs at. \n",
    "\n",
    "- the -c flag handles the nextflow.config automatically created for you \n",
    "\n",
    "- we are using the -profile singularity argument in the above shell script so that Nextflow uses singularity containers when running pipeline \n",
    "\n",
    "- the other arguments are bioinformatics specific and are there to make sure the pipeline runs according to our needs; they were found by visitng the pipeline's documentation page at: https://nf-co.re/methylseq (which you can get to from the nf-core homepage). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb89cc",
   "metadata": {},
   "source": [
    "# Current and Next Steps\n",
    "\n",
    "1.) Implement this same process for Snakemake (do we need/want to?) \n",
    "\n",
    "**Quick note about Nextflow vs Snakemake:** Nextflow is very much like Snakemake in that they both serve the same need, so which one you use is largely up to you and is really preference-based. However, one very notable difference is that Nextflow has an open-source, community built repository of bioinformatics pipelines that you can easily download and use for your own data processing needs so that you don't have to write your own pipelines (but you can if you want to!). This resource is called **nf-core** and is a community effort to collect a curated set of pipelines built using Nextflow. This is nice because it already has many of the workflows and analyses that computational biologists use and so we don't have to waste our efforts and time re-inventing the wheel. Given these advantages and the Core's focus on bioinformatics, we prioritized Nextflow . But I am interested to hear thoughts about adding Snakemake and if it seems necessary or desired. \n",
    "\n",
    "To learn more about nf-core, you can visit its homepage here: \n",
    "\n",
    "https://nf-co.re\n",
    "\n",
    "2.) Creating a GUI for this (Nextflow-tower exists, incorporate this?) and launch with OOD\n",
    "\n",
    "3.) Add GPU option to configuration so that tasks can be run on GPU node\n",
    "\n",
    "4.) Should we limit the number of parallel SLURM jobs? Also, what should max resources be when Nextflow is interacting with OSCAR? \n",
    "\n",
    "5.) Need to do more to handle working and temporary directories for users so they don't have to do much here. \n",
    "\n",
    "6.) Add to the tool to also allow users to easily run their pipelines with just a few inputs after setup and HPC configuration are done. \n",
    "\n",
    "7.) Documentation, documentation, documentation....walk users through configurations and how to use label options to customize workflows even more, singularity containers for custom pipelines, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618e1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
