/**
 * This configuration file specifies the resources for the deployment of a pipeline on
 * our HPC cluster, OSCAR. This file can be modified by the user via python setup script to fit their pipeline needs. 
 * See the Nextflow docs for more information:
 * https://www.nextflow.io/docs/latest/executor.html
 */

params {
  config_profile_description = 'CBC cluster profile'
  config_profile_contact = 'Jordan Lawson (jordan_lawson@brown.edu)'
  //singularity_cache_dir = "$HOME/scratch"
  /**
  * These max resources below act as caps that prevent Nextflow from requesting more resources than
  * can be possibly be allocated for a task on our HPC system to prevent pipeline crashes. 
  * We set max resources to be a little lower than OSCAR's actual maximums  
  */
  max_memory = 200.GB
  max_cpus = 32
  max_time = 24.h
}
singularity {
  enabled = true
  //cacheDir=params.singularity_cache_dir
  autoMounts = true
}
process {
  // these are default settings, which can be changed at setup time or overwritten by specs in main script 
  executor = 'slurm'
  clusterOptions = '-p batch'
  memory = 4.GB  //default mem
  time = 2.h  //default time
  cpus = 2  //default cpus 

// `small_job` describes tasks which require small memory and resources
  withLabel: 'OSCAR_small_job' {
        executor = 'slurm'
        time = 1.h
        memory = 4.GB
        cpus = 1
    }

  // `medium_job` describes tasks which require moderate memory and resources 
  withLabel: 'OSCAR_medium_job' {
        executor = 'slurm'
        time = 16.h
        memory = 8.GB
        cpus = 2
    }

  // `large_job` describes tasks which require lots of memory and resources
  withLabel: 'OSCAR_large_job' {
        executor = 'slurm'
        time = 24.h
        memory = 16.GB
        cpus = 2
    }
}

